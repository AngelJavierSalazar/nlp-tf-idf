{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interesting_Words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1j1s0uIOEnTYceKjYFmHDhS8J_YuOBZlX",
      "authorship_tag": "ABX9TyO6pwoh4reYo7TUZUFzDlBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelJavierSalazar/nlp-tf-idf/blob/main/Interesting_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LbC5Gx7j_6",
        "outputId": "4bf47da1-2fe1-43f4-b228-b3eb5081e266"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import operator\n",
        "import glob, os\n",
        "from collections import defaultdict\n",
        "\n",
        "import spacy\n",
        "import string\n",
        "import unidecode\n",
        "from collections import Counter\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "LApNafTv6mU6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILITY FUNCTIONS "
      ],
      "metadata": {
        "id": "yihNcA2YcQFu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tuples_to_dicts(keys, list_of_tuples):\n",
        "    return [dict(zip(keys, values)) for values in list_of_tuples]"
      ],
      "metadata": {
        "id": "_YUx7jtDYCNV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REFACTORED CLASS DOCUMENT FOR GOOGLE COLAB WITH NLP PROCESSING FUNCTIONS"
      ],
      "metadata": {
        "id": "M1JCsAWAcVcT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        # The name of the file is obtained\n",
        "        self._file_name = os.path.basename(file_path)\n",
        "        # Spacy features to process data in English are loaded\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        # The content of the text file is stored\n",
        "        raw_text = open(file_path, 'r').read()\n",
        "        # Spacy is applied to get a data structure already featurized\n",
        "        spacy_raw_text = nlp(raw_text)\n",
        "        # Each sentence is stored on a list\n",
        "        self._sentences = [str(sentence).strip() for sentence in spacy_raw_text.sents]\n",
        "        # The text is decoded to map uncommon characters to common ones, then transformed into lowercase, and finally punctuation is removed\n",
        "        self._processed_text = nlp(unidecode.unidecode(raw_text).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
        "\n",
        "    def get_most_common_words(self):\n",
        "        return self._common_words_and_freq\n",
        "\n",
        "    def determine_most_common_words(self, amount_of_words):\n",
        "        # If this method is invoked more than once, the value of _common_words_and_freq is calculated only once\n",
        "        if not hasattr(self, '_common_words_and_freq'):\n",
        "            # The words found on the text are stored\n",
        "            words = [chain.text for chain in self._processed_text if chain.pos_ == 'NOUN']\n",
        "            # The frequency of each word is stored with it, as a list of tuples and the 'N' most frequent ones are taken\n",
        "            self._common_words_and_freq = Counter(words).most_common(amount_of_words)\n",
        "            # The list of tuples generated by Counter, is transformed to a list of dictionaries\n",
        "            self._common_words_and_freq = tuples_to_dicts(['value', 'frequency'], self._common_words_and_freq)\n",
        "            # Third and fourth components are added to the dictionary,\n",
        "            # to store the name of the document where the word appear, and the presence of the word on each sentence, as a list of indexes\n",
        "            for word_index, word in enumerate(self._common_words_and_freq):\n",
        "                self._common_words_and_freq[word_index]['file'] = self._file_name\n",
        "                self._common_words_and_freq[word_index]['sentences'] = []\n",
        "\n",
        "    def assign_sentences_to_words(self):\n",
        "        # If the most common words were determined and linked to the sentences where they appear,\n",
        "        # these calculations are not going to be executed again\n",
        "        if hasattr(self, '_common_words_and_freq') and not hasattr(self, '_processed_sentence'):\n",
        "            # Spacy features used to process data in English are loaded\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            # For each sentence, it determines the existence of each of the words\n",
        "            for sentence_index, sentence in enumerate(self._sentences):\n",
        "                # The text is decoded to map uncommon characters to common ones, then transformed into lowercase, and finally punctuation is removed\n",
        "                self._processed_sentence = nlp(unidecode.unidecode(sentence).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
        "                for word_index, word in enumerate(self._common_words_and_freq):\n",
        "                    if word['value'] in [chain.text for chain in self._processed_sentence if chain.pos_ == 'NOUN']:\n",
        "                        # If the the word exist on the sentence, sentence's location on the text is stored on the list of sentences of each word\n",
        "                        self._common_words_and_freq[word_index]['sentences'].append(sentence_index)\n",
        "                        "
      ],
      "metadata": {
        "id": "q-F_o5P9oJYr"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REFACTORED TABLE CLASS FOR GOOGLE COLAB WITH FUNCTIONS TO MERGE WORDS ACROSS DOCS"
      ],
      "metadata": {
        "id": "v5UZcIROcwPo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Table:\n",
        "\n",
        "    def __init__(self, combined_document):\n",
        "        self._entries = []\n",
        "        # A table of entries is defined, based on a merge between the words found on the different documents\n",
        "        for x_index, x in enumerate(combined_document):\n",
        "            if self.word_exists_in_entries(x):\n",
        "                # If the current word is already stored on the table of entries, the values of both are merged\n",
        "                self.update_word_information(x)\n",
        "            else:\n",
        "                # If not, the new word is added to the table of entries\n",
        "                self.insert_word_information(x)\n",
        "\n",
        "    def get_entries(self):\n",
        "        return self._entries\n",
        "\n",
        "    def word_exists_in_entries(self, new_word):\n",
        "        # It returns True is the word already exist on the table of entries, and False otherwise\n",
        "        for stored_word in self._entries:\n",
        "            if stored_word['value'] == new_word['value']:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def insert_word_information(self, new_word):\n",
        "        # It adds the new word to the table of entries\n",
        "        self._entries.append({\n",
        "            'value': new_word['value'],\n",
        "            'frequency': new_word['frequency'],\n",
        "            'files': [new_word['file']],\n",
        "            'sentences_by_file': [new_word['sentences']]\n",
        "        })\n",
        "\n",
        "    def update_word_information(self, new_word):\n",
        "        # It merges the new word with the one already stored on the table of entries,\n",
        "        # adding the frequency of appearance, and appending the list of sentences where the word exist on the current document\n",
        "        for stored_word_index, stored_word in enumerate(self._entries):\n",
        "            if stored_word['value'] == new_word['value']:\n",
        "                self._entries[stored_word_index]['frequency'] += new_word['frequency']\n",
        "                self._entries[stored_word_index]['files'].append(new_word['file'])\n",
        "                self._entries[stored_word_index]['sentences_by_file'].append(new_word['sentences'])"
      ],
      "metadata": {
        "id": "w_WvKTk_q96Z"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS SECTION DETERMINES THE WORD FREQUENCIES"
      ],
      "metadata": {
        "id": "fcpYY6iaYXb8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of most common words to retrieve\n",
        "words_to_retrieve = 10"
      ],
      "metadata": {
        "id": "o8NNelYBhd8s"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Location of the input files\n",
        "files_path = '/content/drive/MyDrive/sample_docs/'\n",
        "# All the file names with extension .txt are retrieved\n",
        "os.chdir(files_path)\n",
        "file_names = []\n",
        "print(file_names)\n",
        "for file_name in glob.glob('*.txt'):\n",
        "    file_names.append(file_name)\n",
        "file_names = sorted(file_names)\n",
        "print(file_names)\n",
        "# A list of documents is created\n",
        "overall_most_common_words = []\n",
        "overall_most_common_words\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mSbyoApYf7e",
        "outputId": "220df931-4711-4ac7-e791-d0fcf9711cbd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt', 'doc6.txt']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The list is filled with Document objects\n",
        "documents = [Document(files_path + file_name) for file_name in file_names]\n",
        "\n"
      ],
      "metadata": {
        "id": "cDnNit0YRGVZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For every file, the significant words are determined, their frequency and the sentences where they appear\n",
        "for document_index, document in enumerate(documents):\n",
        "    print('Processing file \\'' + file_names[document_index] + '\\'...')\n",
        "    document.assign_sentences_to_words()\n",
        "    document.determine_most_common_words(words_to_retrieve)\n",
        "    document.assign_sentences_to_words()\n",
        "    document.determine_most_common_words(words_to_retrieve)\n",
        "    # All the words retrieved from every file, are stored in one list\n",
        "    overall_most_common_words += document.get_most_common_words()\n",
        "# A table of entries is generated, formatted according to the requirements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sf44G1FRQq2",
        "outputId": "6cd6f033-8cab-4f4b-8ee8-0599804e4d26"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 'doc1.txt'...\n",
            "Processing file 'doc2.txt'...\n",
            "Processing file 'doc3.txt'...\n",
            "Processing file 'doc4.txt'...\n",
            "Processing file 'doc5.txt'...\n",
            "Processing file 'doc6.txt'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = Table(overall_most_common_words)\n",
        "table_of_entries = table.get_entries()\n",
        "print('These are the results of word frequencies:', table_of_entries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6pWBY9tROPF",
        "outputId": "4101ca0b-9090-4233-fa78-64a2e5b45024"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the results of word frequencies: [{'value': 'people', 'frequency': 66, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[5, 14, 15, 22, 26, 36, 41, 46, 48, 57, 58, 113, 124], [20, 35, 44, 59, 99, 152, 174, 190, 199, 212], [3, 4, 13, 16, 18, 39, 42, 49, 52, 62, 87, 129], [9, 21, 45, 51, 62, 64, 74, 76, 80, 84, 85, 86, 90, 100, 103, 104, 105, 113, 121], [9, 11, 14, 17, 62, 97, 132, 163, 182]]}, {'value': 'today', 'frequency': 11, 'files': ['doc1.txt'], 'sentences_by_file': [[0, 6, 21, 28, 39, 48, 49, 60, 101, 127, 134]]}, {'value': 'generation', 'frequency': 11, 'files': ['doc1.txt'], 'sentences_by_file': [[38, 39, 74, 78, 82, 85, 87, 88, 92, 93, 108]]}, {'value': 'time', 'frequency': 60, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt', 'doc6.txt'], 'sentences_by_file': [[18, 30, 38, 39, 61, 62, 102, 103, 117, 133], [3, 27, 28, 56, 57, 93, 95, 101, 102, 109, 113, 114, 115, 129, 190, 199], [2, 3, 29, 42, 67, 116, 160, 161, 162, 163, 164, 165, 166], [0, 72, 77, 112, 120], [42, 75, 112, 187, 190, 196, 197, 198, 199], [16, 21, 58, 70]]}, {'value': 'country', 'frequency': 60, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[2, 33, 41, 81, 94, 112], [6, 16, 17, 18, 22, 23, 25, 58, 62, 64, 108, 139, 155, 159, 165, 202], [3, 69, 77, 82, 86, 104, 113, 156, 157, 159, 163], [18, 28, 43, 54, 63, 64, 85, 88, 101, 103, 104, 110, 111, 116, 120], [15, 25, 39, 85, 89, 94, 132, 155, 184, 187, 198]]}, {'value': 'face', 'frequency': 7, 'files': ['doc1.txt'], 'sentences_by_file': [[3, 4, 5, 34, 35, 36, 41]]}, {'value': 'war', 'frequency': 30, 'files': ['doc1.txt', 'doc5.txt'], 'sentences_by_file': [[3, 49, 55, 98, 99, 103, 106], [12, 14, 19, 24, 27, 29, 30, 35, 53, 55, 61, 74, 76, 81, 106, 108, 133, 143, 151, 153, 164, 175]]}, {'value': 'work', 'frequency': 16, 'files': ['doc1.txt', 'doc2.txt'], 'sentences_by_file': [[13, 17, 36, 72, 108, 134], [6, 10, 17, 49, 61, 67, 115, 117, 160, 218]]}, {'value': 'care', 'frequency': 26, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt'], 'sentences_by_file': [[55, 83, 85, 87, 107, 131], [30, 44, 45, 51, 109, 110, 143, 195, 196], [4, 55, 59, 77, 91, 93, 122, 123, 124]]}, {'value': 'heart', 'frequency': 5, 'files': ['doc1.txt'], 'sentences_by_file': [[2, 15, 33, 77, 103]]}, {'value': 'promise', 'frequency': 29, 'files': ['doc2.txt'], 'sentences_by_file': [[6, 8, 9, 21, 62, 74, 75, 76, 77, 78, 82, 83, 108, 109, 115, 118, 123, 124, 169, 206, 207, 208, 225]]}, {'value': 'change', 'frequency': 12, 'files': ['doc2.txt'], 'sentences_by_file': [[2, 26, 29, 84, 85, 86, 136, 147, 188, 189, 190, 192]]}, {'value': 'years', 'frequency': 16, 'files': ['doc2.txt', 'doc6.txt'], 'sentences_by_file': [[5, 8, 15, 18, 23, 66, 91, 92, 162, 208], [20, 25, 36, 47, 57, 64]]}, {'value': 'economy', 'frequency': 10, 'files': ['doc2.txt'], 'sentences_by_file': [[9, 30, 31, 32, 61, 90, 91, 102, 220]]}, {'value': 'government', 'frequency': 39, 'files': ['doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[13, 19, 78, 79, 120, 122, 129, 177, 196], [19, 51, 52, 61, 90, 106, 123, 131], [50, 76, 84, 90, 91, 94, 96, 102], [69, 90, 95, 99, 109, 113, 124, 125, 126, 182]]}, {'value': 'party', 'frequency': 13, 'files': ['doc3.txt'], 'sentences_by_file': [[69, 113, 116, 120, 121, 122, 123, 124, 125, 126, 127, 131, 132]]}, {'value': 't', 'frequency': 12, 'files': ['doc3.txt'], 'sentences_by_file': [[27, 65, 74, 84, 88, 93, 94, 100, 119, 123, 124, 131]]}, {'value': 'health', 'frequency': 9, 'files': ['doc3.txt'], 'sentences_by_file': [[4, 55, 59, 77, 91, 93, 122, 123, 124]]}, {'value': 'world', 'frequency': 19, 'files': ['doc3.txt', 'doc5.txt'], 'sentences_by_file': [[30, 53, 57, 61, 98, 121, 127, 159], [0, 5, 22, 83, 167, 168, 177, 180, 185, 186]]}, {'value': 'school', 'frequency': 6, 'files': ['doc3.txt'], 'sentences_by_file': [[17, 40, 54, 121, 140]]}, {'value': 'corruption', 'frequency': 20, 'files': ['doc4.txt'], 'sentences_by_file': [[51, 56, 57, 62, 65, 67, 68, 69, 71, 73, 74, 75, 77, 79, 86, 94, 104, 106, 110, 111]]}, {'value': 'kenya', 'frequency': 14, 'files': ['doc4.txt'], 'sentences_by_file': [[18, 22, 36, 46, 48, 53, 67, 106, 116, 117, 120, 124]]}, {'value': 'nations', 'frequency': 10, 'files': ['doc4.txt'], 'sentences_by_file': [[32, 37, 40, 43, 47, 48, 50, 53, 123]]}, {'value': 'father', 'frequency': 8, 'files': ['doc4.txt'], 'sentences_by_file': [[11, 14, 17, 19, 119, 121, 124]]}, {'value': 'way', 'frequency': 12, 'files': ['doc4.txt', 'doc6.txt'], 'sentences_by_file': [[53, 69, 92, 93, 108, 114, 125], [13, 32, 35, 67, 69]]}, {'value': 'kenyans', 'frequency': 6, 'files': ['doc4.txt'], 'sentences_by_file': [[27, 52, 63, 73, 86]]}, {'value': 'troops', 'frequency': 20, 'files': ['doc5.txt'], 'sentences_by_file': [[21, 35, 44, 72, 74, 82, 83, 91, 102, 104, 105, 106, 107, 110, 111, 135, 167, 177]]}, {'value': 'strategy', 'frequency': 11, 'files': ['doc5.txt'], 'sentences_by_file': [[25, 26, 66, 67, 76, 90, 118, 123, 129, 150]]}, {'value': 'year', 'frequency': 11, 'files': ['doc5.txt'], 'sentences_by_file': [[30, 35, 36, 37, 38, 39, 78, 92, 191, 192]]}, {'value': 'terrorism', 'frequency': 10, 'files': ['doc5.txt'], 'sentences_by_file': [[8, 12, 58, 110, 139, 140, 147, 150, 168, 175]]}, {'value': 'weapons', 'frequency': 20, 'files': ['doc6.txt'], 'sentences_by_file': [[1, 4, 15, 16, 18, 19, 20, 22, 23, 35, 37, 49, 54, 55, 56, 57, 58, 65, 66]]}, {'value': 'threat', 'frequency': 16, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 15, 22, 23, 25, 31, 35, 37, 48, 49, 51, 65, 67, 79, 80]]}, {'value': 'program', 'frequency': 10, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 4, 24, 29, 36, 39, 48, 49, 69, 75]]}, {'value': 'reduction', 'frequency': 8, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 15, 23, 37, 48, 67, 79, 80]]}, {'value': 'programs', 'frequency': 8, 'files': ['doc6.txt'], 'sentences_by_file': [[15, 17, 36, 37, 47, 62, 71, 74]]}, {'value': 'progress', 'frequency': 6, 'files': ['doc6.txt'], 'sentences_by_file': [[1, 15, 30, 46, 57, 76]]}, {'value': 'security', 'frequency': 6, 'files': ['doc6.txt'], 'sentences_by_file': [[5, 22, 35, 47, 65, 68]]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "7PKY-rVWXSLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint"
      ],
      "metadata": {
        "id": "t25bRmOF0Lzf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entries in table_of_entries:\n",
        "  print(entries)\n"
      ],
      "metadata": {
        "id": "3XPMSjWlpy9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc81faed-7d78-45eb-a515-0a6324b9ccc6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'value': 'people', 'frequency': 66, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[5, 14, 15, 22, 26, 36, 41, 46, 48, 57, 58, 113, 124], [20, 35, 44, 59, 99, 152, 174, 190, 199, 212], [3, 4, 13, 16, 18, 39, 42, 49, 52, 62, 87, 129], [9, 21, 45, 51, 62, 64, 74, 76, 80, 84, 85, 86, 90, 100, 103, 104, 105, 113, 121], [9, 11, 14, 17, 62, 97, 132, 163, 182]]}\n",
            "{'value': 'today', 'frequency': 11, 'files': ['doc1.txt'], 'sentences_by_file': [[0, 6, 21, 28, 39, 48, 49, 60, 101, 127, 134]]}\n",
            "{'value': 'generation', 'frequency': 11, 'files': ['doc1.txt'], 'sentences_by_file': [[38, 39, 74, 78, 82, 85, 87, 88, 92, 93, 108]]}\n",
            "{'value': 'time', 'frequency': 60, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt', 'doc6.txt'], 'sentences_by_file': [[18, 30, 38, 39, 61, 62, 102, 103, 117, 133], [3, 27, 28, 56, 57, 93, 95, 101, 102, 109, 113, 114, 115, 129, 190, 199], [2, 3, 29, 42, 67, 116, 160, 161, 162, 163, 164, 165, 166], [0, 72, 77, 112, 120], [42, 75, 112, 187, 190, 196, 197, 198, 199], [16, 21, 58, 70]]}\n",
            "{'value': 'country', 'frequency': 60, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[2, 33, 41, 81, 94, 112], [6, 16, 17, 18, 22, 23, 25, 58, 62, 64, 108, 139, 155, 159, 165, 202], [3, 69, 77, 82, 86, 104, 113, 156, 157, 159, 163], [18, 28, 43, 54, 63, 64, 85, 88, 101, 103, 104, 110, 111, 116, 120], [15, 25, 39, 85, 89, 94, 132, 155, 184, 187, 198]]}\n",
            "{'value': 'face', 'frequency': 7, 'files': ['doc1.txt'], 'sentences_by_file': [[3, 4, 5, 34, 35, 36, 41]]}\n",
            "{'value': 'war', 'frequency': 30, 'files': ['doc1.txt', 'doc5.txt'], 'sentences_by_file': [[3, 49, 55, 98, 99, 103, 106], [12, 14, 19, 24, 27, 29, 30, 35, 53, 55, 61, 74, 76, 81, 106, 108, 133, 143, 151, 153, 164, 175]]}\n",
            "{'value': 'work', 'frequency': 16, 'files': ['doc1.txt', 'doc2.txt'], 'sentences_by_file': [[13, 17, 36, 72, 108, 134], [6, 10, 17, 49, 61, 67, 115, 117, 160, 218]]}\n",
            "{'value': 'care', 'frequency': 26, 'files': ['doc1.txt', 'doc2.txt', 'doc3.txt'], 'sentences_by_file': [[55, 83, 85, 87, 107, 131], [30, 44, 45, 51, 109, 110, 143, 195, 196], [4, 55, 59, 77, 91, 93, 122, 123, 124]]}\n",
            "{'value': 'heart', 'frequency': 5, 'files': ['doc1.txt'], 'sentences_by_file': [[2, 15, 33, 77, 103]]}\n",
            "{'value': 'promise', 'frequency': 29, 'files': ['doc2.txt'], 'sentences_by_file': [[6, 8, 9, 21, 62, 74, 75, 76, 77, 78, 82, 83, 108, 109, 115, 118, 123, 124, 169, 206, 207, 208, 225]]}\n",
            "{'value': 'change', 'frequency': 12, 'files': ['doc2.txt'], 'sentences_by_file': [[2, 26, 29, 84, 85, 86, 136, 147, 188, 189, 190, 192]]}\n",
            "{'value': 'years', 'frequency': 16, 'files': ['doc2.txt', 'doc6.txt'], 'sentences_by_file': [[5, 8, 15, 18, 23, 66, 91, 92, 162, 208], [20, 25, 36, 47, 57, 64]]}\n",
            "{'value': 'economy', 'frequency': 10, 'files': ['doc2.txt'], 'sentences_by_file': [[9, 30, 31, 32, 61, 90, 91, 102, 220]]}\n",
            "{'value': 'government', 'frequency': 39, 'files': ['doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt'], 'sentences_by_file': [[13, 19, 78, 79, 120, 122, 129, 177, 196], [19, 51, 52, 61, 90, 106, 123, 131], [50, 76, 84, 90, 91, 94, 96, 102], [69, 90, 95, 99, 109, 113, 124, 125, 126, 182]]}\n",
            "{'value': 'party', 'frequency': 13, 'files': ['doc3.txt'], 'sentences_by_file': [[69, 113, 116, 120, 121, 122, 123, 124, 125, 126, 127, 131, 132]]}\n",
            "{'value': 't', 'frequency': 12, 'files': ['doc3.txt'], 'sentences_by_file': [[27, 65, 74, 84, 88, 93, 94, 100, 119, 123, 124, 131]]}\n",
            "{'value': 'health', 'frequency': 9, 'files': ['doc3.txt'], 'sentences_by_file': [[4, 55, 59, 77, 91, 93, 122, 123, 124]]}\n",
            "{'value': 'world', 'frequency': 19, 'files': ['doc3.txt', 'doc5.txt'], 'sentences_by_file': [[30, 53, 57, 61, 98, 121, 127, 159], [0, 5, 22, 83, 167, 168, 177, 180, 185, 186]]}\n",
            "{'value': 'school', 'frequency': 6, 'files': ['doc3.txt'], 'sentences_by_file': [[17, 40, 54, 121, 140]]}\n",
            "{'value': 'corruption', 'frequency': 20, 'files': ['doc4.txt'], 'sentences_by_file': [[51, 56, 57, 62, 65, 67, 68, 69, 71, 73, 74, 75, 77, 79, 86, 94, 104, 106, 110, 111]]}\n",
            "{'value': 'kenya', 'frequency': 14, 'files': ['doc4.txt'], 'sentences_by_file': [[18, 22, 36, 46, 48, 53, 67, 106, 116, 117, 120, 124]]}\n",
            "{'value': 'nations', 'frequency': 10, 'files': ['doc4.txt'], 'sentences_by_file': [[32, 37, 40, 43, 47, 48, 50, 53, 123]]}\n",
            "{'value': 'father', 'frequency': 8, 'files': ['doc4.txt'], 'sentences_by_file': [[11, 14, 17, 19, 119, 121, 124]]}\n",
            "{'value': 'way', 'frequency': 12, 'files': ['doc4.txt', 'doc6.txt'], 'sentences_by_file': [[53, 69, 92, 93, 108, 114, 125], [13, 32, 35, 67, 69]]}\n",
            "{'value': 'kenyans', 'frequency': 6, 'files': ['doc4.txt'], 'sentences_by_file': [[27, 52, 63, 73, 86]]}\n",
            "{'value': 'troops', 'frequency': 20, 'files': ['doc5.txt'], 'sentences_by_file': [[21, 35, 44, 72, 74, 82, 83, 91, 102, 104, 105, 106, 107, 110, 111, 135, 167, 177]]}\n",
            "{'value': 'strategy', 'frequency': 11, 'files': ['doc5.txt'], 'sentences_by_file': [[25, 26, 66, 67, 76, 90, 118, 123, 129, 150]]}\n",
            "{'value': 'year', 'frequency': 11, 'files': ['doc5.txt'], 'sentences_by_file': [[30, 35, 36, 37, 38, 39, 78, 92, 191, 192]]}\n",
            "{'value': 'terrorism', 'frequency': 10, 'files': ['doc5.txt'], 'sentences_by_file': [[8, 12, 58, 110, 139, 140, 147, 150, 168, 175]]}\n",
            "{'value': 'weapons', 'frequency': 20, 'files': ['doc6.txt'], 'sentences_by_file': [[1, 4, 15, 16, 18, 19, 20, 22, 23, 35, 37, 49, 54, 55, 56, 57, 58, 65, 66]]}\n",
            "{'value': 'threat', 'frequency': 16, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 15, 22, 23, 25, 31, 35, 37, 48, 49, 51, 65, 67, 79, 80]]}\n",
            "{'value': 'program', 'frequency': 10, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 4, 24, 29, 36, 39, 48, 49, 69, 75]]}\n",
            "{'value': 'reduction', 'frequency': 8, 'files': ['doc6.txt'], 'sentences_by_file': [[2, 15, 23, 37, 48, 67, 79, 80]]}\n",
            "{'value': 'programs', 'frequency': 8, 'files': ['doc6.txt'], 'sentences_by_file': [[15, 17, 36, 37, 47, 62, 71, 74]]}\n",
            "{'value': 'progress', 'frequency': 6, 'files': ['doc6.txt'], 'sentences_by_file': [[1, 15, 30, 46, 57, 76]]}\n",
            "{'value': 'security', 'frequency': 6, 'files': ['doc6.txt'], 'sentences_by_file': [[5, 22, 35, 47, 65, 68]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(table_of_entries))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOCZTEW7enYc",
        "outputId": "9ccbb696-afd5-4703-82e5-72ab56f2b0b1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_table_of_entries = sorted(table_of_entries, key=lambda x: x['frequency'], reverse=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "CnCXLWY20q-y"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(sorted_table_of_entries)\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-Gcc20Ra2Md2",
        "outputId": "4ba2974f-9116-4198-fa1a-afbcc884f2d5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        value  frequency                                              files  \\\n",
              "0      people         66  [doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....   \n",
              "1        time         60  [doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....   \n",
              "2     country         60  [doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....   \n",
              "3  government         39           [doc2.txt, doc3.txt, doc4.txt, doc5.txt]   \n",
              "4         war         30                               [doc1.txt, doc5.txt]   \n",
              "5     promise         29                                         [doc2.txt]   \n",
              "6        care         26                     [doc1.txt, doc2.txt, doc3.txt]   \n",
              "7  corruption         20                                         [doc4.txt]   \n",
              "8      troops         20                                         [doc5.txt]   \n",
              "9     weapons         20                                         [doc6.txt]   \n",
              "\n",
              "                                   sentences_by_file  \n",
              "0  [[5, 14, 15, 22, 26, 36, 41, 46, 48, 57, 58, 1...  \n",
              "1  [[18, 30, 38, 39, 61, 62, 102, 103, 117, 133],...  \n",
              "2  [[2, 33, 41, 81, 94, 112], [6, 16, 17, 18, 22,...  \n",
              "3  [[13, 19, 78, 79, 120, 122, 129, 177, 196], [1...  \n",
              "4  [[3, 49, 55, 98, 99, 103, 106], [12, 14, 19, 2...  \n",
              "5  [[6, 8, 9, 21, 62, 74, 75, 76, 77, 78, 82, 83,...  \n",
              "6  [[55, 83, 85, 87, 107, 131], [30, 44, 45, 51, ...  \n",
              "7  [[51, 56, 57, 62, 65, 67, 68, 69, 71, 73, 74, ...  \n",
              "8  [[21, 35, 44, 72, 74, 82, 83, 91, 102, 104, 10...  \n",
              "9  [[1, 4, 15, 16, 18, 19, 20, 22, 23, 35, 37, 49...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb62d710-8aa3-4b21-b3c8-bf3fe219dde9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>frequency</th>\n",
              "      <th>files</th>\n",
              "      <th>sentences_by_file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>people</td>\n",
              "      <td>66</td>\n",
              "      <td>[doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....</td>\n",
              "      <td>[[5, 14, 15, 22, 26, 36, 41, 46, 48, 57, 58, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>time</td>\n",
              "      <td>60</td>\n",
              "      <td>[doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....</td>\n",
              "      <td>[[18, 30, 38, 39, 61, 62, 102, 103, 117, 133],...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>country</td>\n",
              "      <td>60</td>\n",
              "      <td>[doc1.txt, doc2.txt, doc3.txt, doc4.txt, doc5....</td>\n",
              "      <td>[[2, 33, 41, 81, 94, 112], [6, 16, 17, 18, 22,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>government</td>\n",
              "      <td>39</td>\n",
              "      <td>[doc2.txt, doc3.txt, doc4.txt, doc5.txt]</td>\n",
              "      <td>[[13, 19, 78, 79, 120, 122, 129, 177, 196], [1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>war</td>\n",
              "      <td>30</td>\n",
              "      <td>[doc1.txt, doc5.txt]</td>\n",
              "      <td>[[3, 49, 55, 98, 99, 103, 106], [12, 14, 19, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>promise</td>\n",
              "      <td>29</td>\n",
              "      <td>[doc2.txt]</td>\n",
              "      <td>[[6, 8, 9, 21, 62, 74, 75, 76, 77, 78, 82, 83,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>care</td>\n",
              "      <td>26</td>\n",
              "      <td>[doc1.txt, doc2.txt, doc3.txt]</td>\n",
              "      <td>[[55, 83, 85, 87, 107, 131], [30, 44, 45, 51, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>corruption</td>\n",
              "      <td>20</td>\n",
              "      <td>[doc4.txt]</td>\n",
              "      <td>[[51, 56, 57, 62, 65, 67, 68, 69, 71, 73, 74, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>troops</td>\n",
              "      <td>20</td>\n",
              "      <td>[doc5.txt]</td>\n",
              "      <td>[[21, 35, 44, 72, 74, 82, 83, 91, 102, 104, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>weapons</td>\n",
              "      <td>20</td>\n",
              "      <td>[doc6.txt]</td>\n",
              "      <td>[[1, 4, 15, 16, 18, 19, 20, 22, 23, 35, 37, 49...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb62d710-8aa3-4b21-b3c8-bf3fe219dde9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb62d710-8aa3-4b21-b3c8-bf3fe219dde9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb62d710-8aa3-4b21-b3c8-bf3fe219dde9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}